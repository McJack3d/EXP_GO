def lire(sujet):
    # Dictionnaire geant de connaissances
    # Utilise des sauts de ligne \n pour lisibilite
    
    if sujet == 1: # SUPERVISE
        print("\n=== SUPERVISED LEARNING ===")
        print("But: Predire Y a partir de X.")
        print("\n1. LINEAR REGRESSION:")
        print("- Hyp: Relation lineaire.")
        print("- Pros: Simple, interpretable.")
        print("- Cons: Sensible outliers, underfitting si non-lin.")
        print("\n2. LOGISTIC REGRESSION:")
        print("- Classification binaire.")
        print("- Sortie: Proba (Sigmoid).")
        print("- Frontiere lineaire.")
        print("\n3. KNN (K-Nearest Neighbors):")
        print("- Instance-based (lazy).")
        print("- Hyperparam K: K petit=Overfit, K grand=Underfit.")
        print("- SCALING OBLIGATOIRE (distances).")
        print("\n4. SVM (Support Vector Machines):")
        print("- Maximize Margin entre classes.")
        print("- Kernel Trick (RBF, Poly) pour non-lineaire.")
        print("- C param: C grand = Hard margin (Overfit risk).")
        print("- C petit = Soft margin (bcp erreurs tolerees).")
        input("Suite...")
        
    elif sujet == 2: # TREES & ENSEMBLE
        print("\n=== TREES & ENSEMBLE ===")
        print("1. DECISION TREES:")
        print("- Non-parametrique, non-lineaire.")
        print("- Pros: Interpretable, pas de scaling.")
        print("- Cons: OVERFITTING (High Variance).")
        print("- Params: max_depth, min_samples_leaf.")
        print("\n2. BAGGING (Random Forest):")
        print("- Parallel.")
        print("- But: REDUIRE VARIANCE (Overfitting).")
        print("- Moyenne de N arbres profonds (independants).")
        print("- Feature Randomness (sqrt features).")
        print("\n3. BOOSTING (Gradient/Ada):")
        print("- Sequentiel.")
        print("- But: REDUIRE BIAIS (Underfitting).")
        print("- Somme de N weak learners (stumps).")
        print("- Chaque arbre corrige erreurs du precedent.")
        print("- Gradient Boost: Fit sur les residus.")
        print("- AdaBoost: Poids eleves sur erreurs.")
        input("Suite...")
        
    elif sujet == 3: # PREPROC & METRICS
        print("\n=== PREPROC & METRICS ===")
        print("SCALING:")
        print("- MinMaxScaler: [0,1]. Sensible outliers.")
        print("- StandardScaler: Mean=0, Std=1. (Z-score).")
        print("- QUI?: KNN, SVM, Lasso/Ridge, NeuralNet.")
        print("- QUI NON?: Trees, Random Forest.")
        print("\nFEATURE SELECTION:")
        print("- Filter: Correlation, Chi2 (rapide).")
        print("- Wrapper: RFE (precis mais lent).")
        print("- Embedded: Lasso, Tree Importance.")
        print("\nMETRICS (Regress):")
        print("- MSE: Penality forte grandes erreurs.")
        print("- MAE: Robuste outliers.")
        print("- R2: % variance expliquee.")
        input("Suite...")
        
    elif sujet == 4: # REGULARIZATION (Question 3)
        print("\n=== REGULARIZATION ===")
        print("But: Reduire Overfitting (Variance) en ajoutant penalite.")
        print("\nRIDGE (L2):")
        print("- Pen: lambda * sum(beta^2)")
        print("- Effet: Shrinkage (coefs -> 0 mais jamais 0).")
        print("- Graph: Cercle. Touche ellipse en un point non-axe.")
        print("- Use: Multicolinearite, garder toutes features.")
        print("\nLASSO (L1):")
        print("- Pen: lambda * sum(|beta|)")
        print("- Effet: Selection (met coefs a 0).")
        print("- Graph: Carre (Losange). Coins sur les axes -> Sparsity.")
        print("- Use: Feature selection auto, model simple.")
        input("Suite...")
        
    elif sujet == 5: # EXAM SPECIFICS
        print("\n=== EXAM TIPS ===")
        print("Q: RF vs Gradient Boosting?")
        print("A: RF=Parallele, Reduit Variance, Arbres profonds.")
        print("   GB=Sequentiel, Reduit Biais, Arbres faibles (stumps).")
        print("\nQ: Housing Price Models?")
        print("1. Linear Reg (Baseline, interpretable).")
        print("2. Random Forest (GÃ¨re non-lin, interactions).")
        print("3. XGBoost (SOTA performance).")
        print("\nQ: Metrics Housing?")
        print("1. RMSE (Meme unite que prix, penalise grosses erreurs).")
        print("2. MAE (Erreur moyenne en $, robuste).")
        print("\nQ: Gini vs Entropy?")
        print("- Gini plus rapide (pas de log).")
        print("- Entropy penalise un peu plus l'impurete.")
        input("Suite...")

def menu():
    print("\n--- THEORIE COMPLETE ---")
    print("1. Supervised (Lin, Log, KNN, SVM)")
    print("2. Trees & Ensembles (RF vs GB)")
    print("3. Preproc & Metrics")
    print("4. Regularization (Lasso/Ridge)")
    print("5. Exam Specifics (Q&A)")
    print("0. Quitter")
    try:
        return int(input("Sujet: "))
    except:
        return -1

while True:
    c = menu()
    if c == 0: break
    elif c in [1,2,3,4,5]: lire(c)